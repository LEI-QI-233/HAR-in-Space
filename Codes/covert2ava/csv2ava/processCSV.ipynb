{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理csv文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证新旧csv文件是否一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "两个 CSV 文件内容完全一致。\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def compare_csv_files(file1, file2):\n",
    "    with open(file1, 'r', newline='', encoding='utf-8') as f1, \\\n",
    "         open(file2, 'r', newline='', encoding='utf-8') as f2:\n",
    "        reader1 = list(csv.reader(f1))\n",
    "        reader2 = list(csv.reader(f2))\n",
    "\n",
    "        if reader1 == reader2:\n",
    "            print(\"两个 CSV 文件内容完全一致。\")\n",
    "        else:\n",
    "            print(\"两个 CSV 文件内容不一致。\")\n",
    "            # 找出不同的地方\n",
    "            max_len = max(len(reader1), len(reader2))\n",
    "            for i in range(max_len):\n",
    "                row1 = reader1[i] if i < len(reader1) else None\n",
    "                row2 = reader2[i] if i < len(reader2) else None\n",
    "                if row1 != row2:\n",
    "                    print(f\"第 {i+1} 行不同：\")\n",
    "                    print(f\"  文件1: {row1}\")\n",
    "                    print(f\"  文件2: {row2}\")\n",
    "\n",
    "# 使用示例\n",
    "compare_csv_files('/home/lqi/lqi_temp/workspace/finished_dataset/csv_files/actions.csv', '/home/lqi/lqi_temp/HAR-in-Space/Dataset/csv_files/actions.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Dataset to AVA-like Format\n",
    "\n",
    "input:  `bounding_boxes.csv`\n",
    "        `frames_root`\n",
    "        `actions.csv`\n",
    "        `output_csv`\n",
    "\n",
    "output: the dataset in one file with ava frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline completed, output saved to ../files/ava_with_head.csv, total records: 13251\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration: define input and output file paths\n",
    "RAW_CSV = '../files/bounding_boxes.csv'\n",
    "FRAMES_ROOT = '/home/lqi/lqi_temp/trainingspace/ava/frames'\n",
    "ACTIONS_CSV = '../files/actions_final.csv'\n",
    "OUTPUT_CSV = '../files/ava_with_head.csv'\n",
    "\n",
    "\n",
    "def load_video_sizes(frames_root: str, video_ids: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preload image dimensions for each video_id by reading the first frame.\n",
    "    Returns a DataFrame with columns: video_id, width, height.\n",
    "    \"\"\"\n",
    "    sizes = []\n",
    "    for vid in video_ids.unique():\n",
    "        folder = os.path.join(frames_root, vid)\n",
    "        if not os.path.isdir(folder):\n",
    "            raise FileNotFoundError(f\"Frames directory not found: {folder}\")\n",
    "        # Find JPEG frames in the directory\n",
    "        frames = [f for f in os.listdir(folder) if f.lower().endswith('.jpg')]\n",
    "        if not frames:\n",
    "            raise ValueError(f\"No jpg frames in {folder}\")\n",
    "        # Open the first image to get its size\n",
    "        with Image.open(os.path.join(folder, frames[0])) as img:\n",
    "            width, height = img.size\n",
    "        sizes.append({'video_id': vid, 'width': width, 'height': height})\n",
    "    return pd.DataFrame(sizes)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Execute the full pipeline:\n",
    "    1. Normalize bounding box coordinates based on frame dimensions.\n",
    "    2. Select the representative frame closest to the median frame_id.\n",
    "    3. Merge with action labels, filter and rename columns.\n",
    "    4. Save the final CSV.\n",
    "    \"\"\"\n",
    "    # 1. Read raw bounding box data\n",
    "    df = pd.read_csv(RAW_CSV, dtype={'xmin': float, 'xmax': float, 'ymin': float, 'ymax': float})\n",
    "\n",
    "    # 2. Preload video dimensions and merge into the DataFrame\n",
    "    sizes_df = load_video_sizes(FRAMES_ROOT, df['video_id'])\n",
    "    df = df.merge(sizes_df, on='video_id', how='left')\n",
    "\n",
    "    # 3. Vectorized normalization of bounding box coordinates\n",
    "    for coord in ['xmin', 'xmax']:\n",
    "        df[coord] = (df[coord] / df['width']).round(3)\n",
    "    for coord in ['ymin', 'ymax']:\n",
    "        df[coord] = (df[coord] / df['height']).round(3)\n",
    "    # Remove auxiliary size columns\n",
    "    df.drop(columns=['width', 'height'], inplace=True)\n",
    "\n",
    "    # 4. Select representative frame: the frame_id closest to the median\n",
    "    df['frame_stamp'] = (\n",
    "        df.groupby(['video_id', 'person_id'])['frame_id']\n",
    "          .transform(lambda x: x.iloc[(x - x.median()).abs().argmin()])\n",
    "    )\n",
    "    df = df[df['frame_id'] == df['frame_stamp']].copy()\n",
    "    df.drop(columns=['frame_id'], inplace=True)\n",
    "\n",
    "    # 5. Merge with action labels and filter out missing actions\n",
    "    df_actions = pd.read_csv(ACTIONS_CSV, usecols=['video_id', 'person_id', 'action'])\n",
    "    df = df.merge(df_actions, on=['video_id', 'person_id'], how='left')\n",
    "    df.dropna(subset=['action'], inplace=True)\n",
    "    df['action'] = df['action'].astype(int)\n",
    "\n",
    "    # 6. Rename and reorder columns for final output\n",
    "    df.rename(columns={'action': 'action_id'}, inplace=True)\n",
    "    df = df[\n",
    "        ['video_id', 'frame_stamp', 'xmin', 'ymin', 'xmax', 'ymax', 'action_id', 'person_id']\n",
    "    ]\n",
    "\n",
    "    # 7. Save the final CSV file\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"✅ Pipeline completed, output saved to {OUTPUT_CSV}, total records: {len(df)}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建训练集，验证集，测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接生成ava官方格式的csv文件用于annotation文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "按视频划分完成：行级统计：\n",
      "  Train: 9266 条，占总 9266/13251 = 69.93%\n",
      "  Val: 1329 条，占总 1329/13251 = 10.03%\n",
      "  Test: 2656 条，占总 2656/13251 = 20.04%\n",
      "按视频划分完成：视频级统计：\n",
      "  Train: 3331 个视频，占总 3331/4759 = 69.99%\n",
      "  Val: 475 个视频，占总 475/4759 = 9.98%\n",
      "  Test: 953 个视频，占总 953/4759 = 20.03%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "按视频划分带动作覆盖：\n",
    "  - 在视频级别贪心挑选视频，保证训练集至少包含每个 action_id；\n",
    "  - 然后对剩余视频按比例 7:1:2 随机划分到 train/val/test；\n",
    "  - 输出 train.csv、val.csv、test.csv（无表头），同一视频的所有行不跨集；\n",
    "  - 输出到同一指定文件夹，若不存在会自动创建；\n",
    "  - 运行结束时打印每个子集的行数、行占比；以及视频数、视频占比。\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# —— 配置区 —— \n",
    "INPUT_CSV = \"../files/ava_with_head.csv\"\n",
    "OUTPUT_DIR = \"../files/output/ava_dataset\"  # train.csv、val.csv、test.csv 都会放在这里\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO   = 0.1\n",
    "TEST_RATIO  = 0.2\n",
    "SEED        = 42\n",
    "\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    \"\"\"如果目录不存在，就创建它。\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def split_by_video_with_action_coverage(\n",
    "    input_csv: str,\n",
    "    output_dir: str,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.2,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    # 1. 载入原始标注\n",
    "    df = pd.read_csv(input_csv)\n",
    "    total_rows = len(df)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 2. 视频级别映射 video_id -> action_id 列表\n",
    "    vid2acts = df.groupby('video_id')['action_id'].unique().to_dict()\n",
    "    all_videos = list(vid2acts.keys())\n",
    "    total_videos = len(all_videos)\n",
    "\n",
    "    # 3. 计算视频级目标数\n",
    "    train_v_target = int(train_ratio * total_videos)\n",
    "    val_v_target   = int(val_ratio   * total_videos)\n",
    "\n",
    "    # 4. 贪心挑视频覆盖所有动作\n",
    "    all_actions = set(df['action_id'].unique())\n",
    "    covered = set()\n",
    "    selected_videos = set()\n",
    "    while covered != all_actions:\n",
    "        best_vid, best_gain = None, 0\n",
    "        for vid, acts in vid2acts.items():\n",
    "            if vid in selected_videos:\n",
    "                continue\n",
    "            gain = len(set(acts) - covered)\n",
    "            if gain > best_gain:\n",
    "                best_gain, best_vid = gain, vid\n",
    "        if best_vid is None:\n",
    "            break\n",
    "        selected_videos.add(best_vid)\n",
    "        covered |= set(vid2acts[best_vid])\n",
    "\n",
    "    # 5. 剩余视频随机划分\n",
    "    remaining = [v for v in all_videos if v not in selected_videos]\n",
    "    np.random.shuffle(remaining)\n",
    "    need_more = max(train_v_target - len(selected_videos), 0)\n",
    "    more_train = remaining[:need_more]\n",
    "    val_videos  = remaining[need_more: need_more + val_v_target]\n",
    "    test_videos = remaining[need_more + val_v_target:]\n",
    "\n",
    "    # 最终子集视频列表\n",
    "    train_videos = set(selected_videos) | set(more_train)\n",
    "    val_videos   = set(val_videos)\n",
    "    test_videos  = set(test_videos)\n",
    "\n",
    "    # 6. 根据 video_id 划分行\n",
    "    df_train = df[df['video_id'].isin(train_videos)]\n",
    "    df_val   = df[df['video_id'].isin(val_videos)]\n",
    "    df_test  = df[df['video_id'].isin(test_videos)]\n",
    "\n",
    "    # 7. 保存至输出目录（无表头）\n",
    "    ensure_dir(output_dir)\n",
    "    paths = {\n",
    "        'train': os.path.join(output_dir, 'ava_train.csv'),\n",
    "        'val':   os.path.join(output_dir, 'ava_val.csv'),\n",
    "        'test':  os.path.join(output_dir, 'ava_test.csv'),\n",
    "    }\n",
    "    df_train.to_csv(paths['train'], index=False, header=False)\n",
    "    df_val.to_csv(  paths['val'],   index=False, header=False)\n",
    "    df_test.to_csv( paths['test'],  index=False, header=False)\n",
    "\n",
    "    # 8. 打印行级统计\n",
    "    print(\"按视频划分完成：行级统计：\")\n",
    "    for name, subdf in [('Train', df_train), ('Val', df_val), ('Test', df_test)]:\n",
    "        cnt = len(subdf)\n",
    "        print(f\"  {name}: {cnt} 条，占总 {cnt}/{total_rows} = {cnt/total_rows*100:.2f}%\")\n",
    "\n",
    "    # 9. 打印视频级统计\n",
    "    print(\"按视频划分完成：视频级统计：\")\n",
    "    for name, vids in [('Train', train_videos), ('Val', val_videos), ('Test', test_videos)]:\n",
    "        vcnt = len(vids)\n",
    "        print(f\"  {name}: {vcnt} 个视频，占总 {vcnt}/{total_videos} = {vcnt/total_videos*100:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    split_by_video_with_action_coverage(\n",
    "        INPUT_CSV,\n",
    "        OUTPUT_DIR,\n",
    "        train_ratio=TRAIN_RATIO,\n",
    "        val_ratio=VAL_RATIO,\n",
    "        test_ratio=TEST_RATIO,\n",
    "        seed=SEED,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按视频划分完成：行级统计：\n",
    "  Train: 9266 条，占总 9266/13251 = 69.93%\n",
    "  Val: 1329 条，占总 1329/13251 = 10.03%\n",
    "  Test: 2656 条，占总 2656/13251 = 20.04%\n",
    "按视频划分完成：视频级统计：\n",
    "  Train: 3331 个视频，占总 3331/4759 = 69.99%\n",
    "  Val: 475 个视频，占总 475/4759 = 9.98%\n",
    "  Test: 953 个视频，占总 953/4759 = 20.03%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成frame lists里面的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成 ../files/output/new_frame_lists/train.csv\n",
      "生成 ../files/output/new_frame_lists/val.csv\n",
      "生成 ../files/output/new_frame_lists/test.csv\n",
      "所有 frame_lists 生成完毕！\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "根据已有的 ava_train/ava_val/ava_test 标注 CSV 和 frames 目录，\n",
    "生成 AVA 所需的 frame_lists/{train,val,test}.csv：\n",
    "  - 输入标注文件名：ava_train.csv, ava_val.csv, ava_test.csv（无表头）；\n",
    "  - 输出文件名：train.csv, val.csv, test.csv（带表头）；\n",
    "  - 列格式：original_video_id video_id frame_id path labels；\n",
    "  - path 格式：\"{video_id}/{video_id}_{frame_id:06d}.jpg\"；\n",
    "  - labels 列固定填 \"\"；\n",
    "  - 输出目录不存在会自动创建。\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# —— 配置区 —— \n",
    "ANNOT_DIR     = \"../files/output/ava_dataset\"   # 这里存放 ava_train.csv, ava_val.csv, ava_test.csv\n",
    "FRAMES_ROOT   = \"/home/lqi/lqi_temp/trainingspace/ava/frames\"        # 这里每个子目录是一个 video_id，内含 .jpg\n",
    "OUTPUT_DIR    = \"../files/output/new_frame_lists\"   # 生成 train.csv, val.csv, test.csv 到此目录\n",
    "SPLITS        = [\"train\", \"val\", \"test\"]\n",
    "INPUT_PREFIX  = \"ava_\"                       # 标注文件名前缀\n",
    "HEADER        = \"original_video_id video_id frame_id path labels\\n\"\n",
    "\n",
    "def ensure_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def load_split_videos(split):\n",
    "    \"\"\"读取 ava_{split}.csv，返回该 split 包含的所有 video_id 集合\"\"\"\n",
    "    csv_path = os.path.join(ANNOT_DIR, f\"{INPUT_PREFIX}{split}.csv\")\n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        header=None,\n",
    "        names=[\"video_id\",\"frame_stamp\",\"xmin\",\"ymin\",\"xmax\",\"ymax\",\"action_id\",\"person_id\"],\n",
    "    )\n",
    "    return set(df[\"video_id\"].astype(str).unique())\n",
    "\n",
    "def make_frame_list(split, videoset):\n",
    "    \"\"\"为一个 split 枚举所有 frames，写入 frame_lists/{split}.csv\"\"\"\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"{split}.csv\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        fout.write(HEADER)\n",
    "        for vid in sorted(videoset):\n",
    "            vid_dir = os.path.join(FRAMES_ROOT, vid)\n",
    "            pattern = os.path.join(vid_dir, f\"{vid}_*.jpg\")\n",
    "            for img_path in sorted(glob.glob(pattern)):\n",
    "                fname = os.path.basename(img_path)\n",
    "                # 提取帧编号\n",
    "                frame_id = int(fname.replace(f\"{vid}_\", \"\").replace(\".jpg\", \"\"))\n",
    "                relpath = f\"{vid}/{fname}\"\n",
    "                # labels 列填 \"\"\n",
    "                fout.write(f\"{vid} {vid} {frame_id} {relpath} \\\"\\\"\\n\")\n",
    "    print(f\"生成 {out_path}\")\n",
    "\n",
    "def main():\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "    for split in SPLITS:\n",
    "        vids = load_split_videos(split)\n",
    "        make_frame_list(split, vids)\n",
    "    print(\"所有 frame_lists 生成完毕！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成test和val的annotation文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理：ava_val.csv → ava_val_predicted_boxes.csv\n",
      "已处理：ava_train.csv → ava_train_predicted_boxes.csv\n",
      "已处理：ava_test.csv → ava_test_predicted_boxes.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_csv_folder(input_folder, output_folder='../files/output/ava_dataset'):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for fname in os.listdir(input_folder):\n",
    "        if not fname.lower().endswith('.csv'):\n",
    "            continue\n",
    "        in_path = os.path.join(input_folder, fname)\n",
    "        \n",
    "        # 1. 读取 CSV（无表头，确保每列都当字符串）\n",
    "        df = pd.read_csv(in_path, header=None, dtype=str, sep=',')\n",
    "        \n",
    "        # 2. 跳过列数不足两列的文件\n",
    "        if df.shape[1] < 2:\n",
    "            print(f\"跳过 {fname}：列数不足\")\n",
    "            continue\n",
    "        \n",
    "        # 3. 删除最后两列\n",
    "        df = df.iloc[:, :-2]\n",
    "        \n",
    "        # 4. 添加倒数第二列（全空）和最后一列（全 '1'）\n",
    "        #    这里我们给它们不同的列名，避免冲突\n",
    "        df['blank_col'] = ''\n",
    "        df['fill11_col'] = '1'\n",
    "        \n",
    "        # 5. 保存到 output 文件夹，不写列名、不写索引\n",
    "        base, _ = os.path.splitext(fname)\n",
    "        out_name = f\"{base}_predicted_boxes.csv\"\n",
    "        df.to_csv(\n",
    "            os.path.join(output_folder, out_name),\n",
    "            header=False,    # 不输出列名\n",
    "            index=False,     # 不输出行索引\n",
    "            sep=','\n",
    "        )\n",
    "        print(f\"已处理：{fname} → {out_name}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 把下面这个路径改成你自己的 CSV 文件所在目录\n",
    "    input_folder = '../files/output/ava_dataset'\n",
    "    process_csv_folder(input_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
